{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3tb/Hell/hell/Anaconda3/envs/ML/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/data3tb/Hell/hell/Anaconda3/envs/ML/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load numpy, pandas, sklearn, torch, etc\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch import *\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('/home/fhell/Desktop/verkehrsunfaelle_train.csv',encoding ='latin1')\n",
    "test_df = pd.read_csv('/home/fhell/Desktop/ittalents/verkehrsunfaelle_test.csv',encoding ='latin1')\n",
    "\n",
    "#make copy of original df\n",
    "X_train_1=train_df\n",
    "X_test_1=test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle outlier\n",
    "29 of Feb which parser cant handle, year is not given in most entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0,X_train_1.shape[0]):\n",
    "    if X_train_1['Unfalldatum'][idx] ==  '29. Feb.':\n",
    "        X_train_1['Unfalldatum'][idx] ='28. Feb.'\n",
    "\n",
    "for idx in range(0,X_test_1.shape[0]):\n",
    "    if X_test_1['Unfalldatum'][idx] ==  '29. Feb.':\n",
    "        X_test_1['Unfalldatum'][idx] ='28. Feb.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse datetime from dataframe \n",
    "using dateutil.parser, dateutil parser has to be edited to account for german date names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0,train_df.shape[0]):\n",
    "    X_train_1['Unfalldatum'][idx] = parse(X_train_1['Unfalldatum'][idx])\n",
    "    \n",
    "for idx in range(0,X_test_1.shape[0]):\n",
    "    X_test_1['Unfalldatum'][idx] = parse(X_test_1['Unfalldatum'][idx])\n",
    "    \n",
    "# extract month, as most year values are missing in the original dataset, we neglect year (we could also create a distribution from the not missing data and fill the missing values)\n",
    "X_train_1['Monat'] = X_train_1['Unfalldatum'].dt.month\n",
    "X_test_1['Monat'] = X_test_1['Unfalldatum'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data after parsing\n",
    "\n",
    "X_train_1.to_csv('train.csv',encoding ='latin1')\n",
    "X_test_1.to_csv('test.csv',encoding ='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parsed data again\n",
    "train_df = pd.read_csv('/home/fhell/Desktop/train.csv',encoding ='latin1')\n",
    "test_df = pd.read_csv('/home/fhell/Desktop/test.csv',encoding ='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strassenklasse</th>\n",
       "      <th>Alter</th>\n",
       "      <th>Unfallklasse</th>\n",
       "      <th>Unfallschwere</th>\n",
       "      <th>Lichtverhältnisse</th>\n",
       "      <th>Verletzte Personen</th>\n",
       "      <th>Anzahl Fahrzeuge</th>\n",
       "      <th>Bodenbeschaffenheit</th>\n",
       "      <th>Geschlecht</th>\n",
       "      <th>Zeit (24h)</th>\n",
       "      <th>Fahrzeugtyp</th>\n",
       "      <th>Wetterlage</th>\n",
       "      <th>Monat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bundesstrasse</td>\n",
       "      <td>59</td>\n",
       "      <td>Fahrer</td>\n",
       "      <td>1</td>\n",
       "      <td>Tageslicht: Strassenbeleuchtung vorhanden</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>trocken</td>\n",
       "      <td>männlich</td>\n",
       "      <td>1330</td>\n",
       "      <td>LKW ab 7.5t</td>\n",
       "      <td>Gut</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autobahn</td>\n",
       "      <td>48</td>\n",
       "      <td>Fahrer</td>\n",
       "      <td>1</td>\n",
       "      <td>Tageslicht: Strassenbeleuchtung vorhanden</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trocken</td>\n",
       "      <td>weiblich</td>\n",
       "      <td>1724</td>\n",
       "      <td>Auto</td>\n",
       "      <td>Gut</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nicht klassifiziert</td>\n",
       "      <td>56</td>\n",
       "      <td>Fahrer</td>\n",
       "      <td>2</td>\n",
       "      <td>Tageslicht: Strassenbeleuchtung vorhanden</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>nass / feucht</td>\n",
       "      <td>männlich</td>\n",
       "      <td>1345</td>\n",
       "      <td>Mottorrad (500cc)</td>\n",
       "      <td>Gut</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bundesstrasse</td>\n",
       "      <td>66</td>\n",
       "      <td>Fahrer</td>\n",
       "      <td>1</td>\n",
       "      <td>Dunkelheit: Strassenbeleuchtung vorhanden und ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>nass / feucht</td>\n",
       "      <td>weiblich</td>\n",
       "      <td>1830</td>\n",
       "      <td>Auto</td>\n",
       "      <td>Regen</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bundesstrasse</td>\n",
       "      <td>33</td>\n",
       "      <td>Fahrer</td>\n",
       "      <td>2</td>\n",
       "      <td>Dunkelheit: keine Strassenbeleuchtung</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>nass / feucht</td>\n",
       "      <td>männlich</td>\n",
       "      <td>15</td>\n",
       "      <td>Auto</td>\n",
       "      <td>Gut</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Strassenklasse  Alter Unfallklasse  Unfallschwere  \\\n",
       "0        Bundesstrasse     59       Fahrer              1   \n",
       "1             Autobahn     48       Fahrer              1   \n",
       "2  nicht klassifiziert     56       Fahrer              2   \n",
       "3        Bundesstrasse     66       Fahrer              1   \n",
       "4        Bundesstrasse     33       Fahrer              2   \n",
       "\n",
       "                                   Lichtverhältnisse  Verletzte Personen  \\\n",
       "0          Tageslicht: Strassenbeleuchtung vorhanden                   2   \n",
       "1          Tageslicht: Strassenbeleuchtung vorhanden                   2   \n",
       "2          Tageslicht: Strassenbeleuchtung vorhanden                   1   \n",
       "3  Dunkelheit: Strassenbeleuchtung vorhanden und ...                   3   \n",
       "4              Dunkelheit: keine Strassenbeleuchtung                   1   \n",
       "\n",
       "   Anzahl Fahrzeuge Bodenbeschaffenheit Geschlecht  Zeit (24h)  \\\n",
       "0                 2             trocken   männlich        1330   \n",
       "1                 4             trocken   weiblich        1724   \n",
       "2                 1       nass / feucht   männlich        1345   \n",
       "3                 2       nass / feucht   weiblich        1830   \n",
       "4                 1       nass / feucht   männlich          15   \n",
       "\n",
       "         Fahrzeugtyp Wetterlage  Monat  \n",
       "0        LKW ab 7.5t        Gut      1  \n",
       "1               Auto        Gut      5  \n",
       "2  Mottorrad (500cc)        Gut      2  \n",
       "3               Auto      Regen     10  \n",
       "4               Auto        Gut     11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make copy of original df\n",
    "X_train_1=train_df\n",
    "X_test_1=test_df\n",
    "\n",
    "# Drop useless variables \n",
    "X_train_1.drop(labels = ['Unnamed: 0','Unfalldatum'], axis = 1, inplace = True)\n",
    "X_test_1.drop(labels = ['Unnamed: 0','Unfalldatum'], axis = 1, inplace = True)\n",
    "X_train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse 24h and extract hour\n",
    "X_train_1['Zeit (24h)']=X_train_1['Zeit (24h)'].apply(lambda x: '{0:0>4}'.format(x))\n",
    "X_test_1['Zeit (24h)']=X_test_1['Zeit (24h)'].apply(lambda x: '{0:0>4}'.format(x))\n",
    "X_test_1['Zeit (24h)'] = pd.to_datetime(X_test_1['Zeit (24h)'], format = '%H%M')\n",
    "X_test_1['Zeit (24h)'] = X_test_1['Zeit (24h)'].dt.hour\n",
    "X_train_1['Zeit (24h)'] = pd.to_datetime(X_train_1['Zeit (24h)'], format = '%H%M')\n",
    "X_train_1['Zeit (24h)'] = X_train_1['Zeit (24h)'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## month is a cyclic feature. hence some cyclic feature engineering, see https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "m_per_year = 12\n",
    "\n",
    "X_train_1['sin Monat'] = np.sin(2*np.pi*X_train_1['Monat']/m_per_year)\n",
    "X_train_1['cos Monat'] = np.cos(2*np.pi*X_train_1['Monat']/m_per_year)\n",
    "X_test_1['sin Monat'] = np.sin(2*np.pi*X_test_1['Monat']/m_per_year)\n",
    "X_test_1['cos Monat'] = np.cos(2*np.pi*X_test_1['Monat']/m_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding\n",
    "we chose label encoding on the original data, we do not create additional features from e.g. 'Lichtervältnisse'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "\n",
    "columns = [\n",
    " 'Strassenklasse',\n",
    " 'Unfallklasse',\n",
    " 'Lichtverhältnisse',\n",
    " 'Bodenbeschaffenheit',\n",
    " 'Geschlecht',\n",
    " 'Fahrzeugtyp',\n",
    " 'Wetterlage']\n",
    "\n",
    "for col in columns:\n",
    "\n",
    "       if train_df[col].dtypes=='object':\n",
    "        data=train_df[col].append(test_df[col])\n",
    "        le.fit(data.values)\n",
    "        train_df[col]=le.transform(train_df[col])\n",
    "        test_df[col]=le.transform(test_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['Unfallschwere']\n",
    "\n",
    "enc=OneHotEncoder(sparse=False)\n",
    "\n",
    "for col in columns:\n",
    "    data=X_train_1[[col]]\n",
    "    enc.fit(data)\n",
    "\n",
    "    temp = enc.transform(X_train_1[[col]])\n",
    "\n",
    "    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data[col]\n",
    "            .value_counts().index])\n",
    "\n",
    "    temp=temp.set_index(X_train_1.index.values) \n",
    "    Y_train_1=pd.concat([temp],axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization \n",
    "for Age and other columns, we do not bin age groups, which might be useful: https://arxiv.org/pdf/1702.04415.pdf\n",
    "could use embeddings instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['Alter', 'Verletzte Personen',\n",
    " 'Anzahl Fahrzeuge']\n",
    "\n",
    "for col in columns:\n",
    "    data=X_train_1[[col]].append(X_test_1[[col]])\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(data)\n",
    "    \n",
    "    temp = scaler.transform(X_train_1[[col]])\n",
    "\n",
    "    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str('scaled'))])\n",
    "\n",
    "    temp=temp.set_index(X_train_1.index.values)\n",
    "       \n",
    "    X_train_1=pd.concat([X_train_1,temp],axis=1)\n",
    "\n",
    "    temp = scaler.transform(X_test_1[[col]])\n",
    "       \n",
    "    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str('scaled'))])\n",
    "\n",
    "\n",
    "    temp=temp.set_index(X_test_1.index.values)\n",
    "\n",
    "    X_test_1=pd.concat([X_test_1,temp],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_traintemp = Y_train_1\n",
    "X_traintemp = X_train_1.drop(labels = [\"Unfallschwere\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categorial and continous features\n",
    "we chose to embed daytime, while we use cyclic feature engineering for month - out of curiosity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Strassenklasse',\n",
    "\n",
    " 'Unfallklasse',\n",
    " 'Lichtverhältnisse',\n",
    " 'Zeit (24h)',\n",
    " 'Geschlecht',\n",
    "\n",
    " 'Bodenbeschaffenheit',\n",
    "\n",
    " 'Fahrzeugtyp',\n",
    " 'Wetterlage']\n",
    "\n",
    "cont_features = [\n",
    " 'Verletzte Personen_scaled',\n",
    " 'Anzahl Fahrzeuge_scaled',\n",
    "\n",
    " 'Alter_scaled',\n",
    " 'sin Monat',\n",
    " 'cos Monat',\n",
    "\n",
    "                       ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct embedding\n",
    "how many unique values are in training and test dataset per feature, construct embedding dimensionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6, 3, 5, 24, 2, 7, 17, 9],\n",
       " [(6, 3), (3, 2), (5, 3), (24, 12), (2, 1), (7, 4), (17, 9), (9, 5)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many unique values are in training and test dataset per categorial feature, construct embedding matrix\n",
    "tempc = pd.concat([X_traintemp,X_test_1])\n",
    "\n",
    "cat_dims = [int(tempc[col].nunique()) for col in categorical_features]\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "\n",
    "cat_dims, emb_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data set into training and validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_test, Y_train, y_test = train_test_split(X_traintemp, Y_traintemp, test_size=0.2, random_state=107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample minority class\n",
    "a more advance data augmentation technique would be useful here such as using GANs (https://stats.stackexchange.com/questions/358970/can-a-gan-be-used-for-tabular-vector-data-augmentation) \n",
    "or Synthetic Minority Over-sampling Technique (SMOTE) (Chawla et al., 2012),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32289, 3), (32289, 17))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatemp = pd.concat([pd.Series.astype(X_train,dtype = np.float64),pd.Series.astype(Y_train,dtype = np.float64)],axis=1)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_1 = datatemp[datatemp['Unfallschwere_1'].values==1]\n",
    "df_2 = datatemp[datatemp['Unfallschwere_2'].values==1]\n",
    "df_3 = datatemp[datatemp['Unfallschwere_3'].values==1]\n",
    "\n",
    " \n",
    "# Upsample minority class 2\n",
    "df_2_upsampled = resample(df_2, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=df_1.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Upsample minority class 3\n",
    "df_3_upsampled = resample(df_3, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=df_1.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_1, df_2_upsampled, df_3_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled[df_upsampled['Unfallschwere_1'].values==1].shape, df_upsampled[df_upsampled['Unfallschwere_3'].values==1].shape\n",
    "\n",
    "## seperate features from labels again\n",
    "Y_train = df_upsampled[[\"Unfallschwere_1\",\"Unfallschwere_2\",\"Unfallschwere_3\"]]\n",
    "X_train = df_upsampled.drop(labels = [\"Unfallschwere_1\",\"Unfallschwere_2\",\"Unfallschwere_3\"],axis = 1)\n",
    "Y_train.shape,X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate continous and categorial features, construct dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate continous and categorial features for dataloader\n",
    "cont = X_train[cont_features]\n",
    "cat = X_train[categorical_features]\n",
    "\n",
    "#change datatype to torch tensor, create dataset and dataloader with batches\n",
    "\n",
    "#prepare train data for pytorch (categorial features are int, while continous are float)\n",
    "Y_tensor_train  = torch.tensor(np.asarray(Y_train.values,dtype=np.float32))\n",
    "X_tensor_train  = torch.tensor(np.asarray(cat.values,dtype=np.int64))\n",
    "cont_data = torch.tensor(np.asarray(cont.values,dtype=np.float32))\n",
    "\n",
    "dataset = TensorDataset(X_tensor_train, Y_tensor_train, cont_data)\n",
    "dataloader = DataLoader(dataset, batch_size=100,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "#prepare validation data\n",
    "cont_val = x_test[cont_features]\n",
    "cat_val = x_test[categorical_features]\n",
    "X_tensor_val  = torch.tensor(np.asarray(cat_val.values,dtype=np.int64))\n",
    "Y_tensor_val  = torch.tensor(np.asarray(y_test.values,dtype=np.float32))\n",
    "cont_data_val = torch.tensor(np.asarray(cont_val.values,dtype=np.float32))  \n",
    "\n",
    "#prepare test data\n",
    "cont_test = X_test_1[cont_features]\n",
    "cat_test = X_test_1[categorical_features]\n",
    "X_tensor_test  = torch.tensor(np.asarray(cat_test.values,dtype=np.int64))\n",
    "cont_data_test = torch.tensor(np.asarray(cont_test.values,dtype=np.float32))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify neural network parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3tb/Hell/hell/Anaconda3/envs/ML/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/data3tb/Hell/hell/Anaconda3/envs/ML/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/data3tb/Hell/hell/Anaconda3/envs/ML/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/data3tb/Hell/hell/Anaconda3/envs/ML/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/data3tb/Hell/hell/Anaconda3/envs/ML/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_FEATURES =  X_train_1.shape[1]\n",
    "LR = 0.001\n",
    "#different dropout for different layers, more dropout for later layers\n",
    "dropout = torch.nn.Dropout(p=1 - (0.5))\n",
    "dropout1 = torch.nn.Dropout(p=1 - (0.9))\n",
    "dropout2 = torch.nn.Dropout(p=1 - (0.75))\n",
    "no_of_cont = cont_data.shape[1]\n",
    "\n",
    "N_LABELS = Y_train_1.shape[1]   #3 #n classes\n",
    "\n",
    "\n",
    "hiddenLayer1Size=512\n",
    "hiddenLayer2Size=int(hiddenLayer1Size/2)\n",
    "hiddenLayer3Size=int(hiddenLayer1Size/4)\n",
    "hiddenLayer4Size=int(hiddenLayer1Size/8)\n",
    "hiddenLayer5Size=int(hiddenLayer1Size/16)\n",
    "\n",
    "emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
    "                                     for x, y in emb_dims])\n",
    "no_of_embs = sum([y for x, y in emb_dims])\n",
    "bn1 = nn.BatchNorm1d(no_of_cont)\n",
    "\n",
    "linear1=torch.nn.Linear(no_of_embs+no_of_cont, hiddenLayer1Size, bias=True) \n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "\n",
    "linear2=torch.nn.Linear(hiddenLayer1Size, hiddenLayer2Size)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "\n",
    "linear3=torch.nn.Linear(hiddenLayer2Size, hiddenLayer3Size)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "\n",
    "linear4=torch.nn.Linear(hiddenLayer3Size, hiddenLayer4Size)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "\n",
    "linear5=torch.nn.Linear(hiddenLayer4Size, N_LABELS)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)\n",
    "\n",
    "\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "sftmx = torch.nn.Softmax()\n",
    "tanh=torch.nn.Tanh()\n",
    "leakyrelu=torch.nn.LeakyReLU()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct classifier, forward pass, run training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1500] Loss: 0.642\n",
      "[1/1500] Validation log loss: 12.595\n",
      "[2/1500] Loss: 0.493\n",
      "[3/1500] Loss: 0.416\n",
      "[4/1500] Loss: 0.371\n",
      "[5/1500] Loss: 0.343\n",
      "[6/1500] Loss: 0.328\n",
      "[7/1500] Loss: 0.314\n",
      "[8/1500] Loss: 0.303\n",
      "[9/1500] Loss: 0.293\n",
      "[10/1500] Loss: 0.285\n",
      "[11/1500] Loss: 0.284\n",
      "[11/1500] Validation log loss: 18.508\n",
      "[12/1500] Loss: 0.273\n",
      "[13/1500] Loss: 0.265\n",
      "[14/1500] Loss: 0.263\n",
      "[15/1500] Loss: 0.256\n",
      "[16/1500] Loss: 0.254\n",
      "[17/1500] Loss: 0.251\n",
      "[18/1500] Loss: 0.246\n",
      "[19/1500] Loss: 0.242\n",
      "[20/1500] Loss: 0.241\n",
      "[21/1500] Loss: 0.235\n",
      "[21/1500] Validation log loss: 19.322\n",
      "[22/1500] Loss: 0.232\n",
      "[23/1500] Loss: 0.229\n",
      "[24/1500] Loss: 0.225\n",
      "[25/1500] Loss: 0.223\n",
      "[26/1500] Loss: 0.222\n",
      "[27/1500] Loss: 0.218\n",
      "[28/1500] Loss: 0.217\n",
      "[29/1500] Loss: 0.215\n",
      "[30/1500] Loss: 0.213\n",
      "[31/1500] Loss: 0.209\n",
      "[31/1500] Validation log loss: 17.999\n",
      "[32/1500] Loss: 0.206\n",
      "[33/1500] Loss: 0.203\n",
      "[34/1500] Loss: 0.205\n",
      "[35/1500] Loss: 0.204\n",
      "[36/1500] Loss: 0.198\n",
      "[37/1500] Loss: 0.199\n",
      "[38/1500] Loss: 0.195\n",
      "[39/1500] Loss: 0.194\n",
      "[40/1500] Loss: 0.198\n",
      "[41/1500] Loss: 0.194\n",
      "[41/1500] Validation log loss: 15.696\n",
      "[42/1500] Loss: 0.194\n",
      "[43/1500] Loss: 0.191\n",
      "[44/1500] Loss: 0.189\n",
      "[45/1500] Loss: 0.185\n",
      "[46/1500] Loss: 0.185\n",
      "[47/1500] Loss: 0.186\n",
      "[48/1500] Loss: 0.181\n",
      "[49/1500] Loss: 0.183\n",
      "[50/1500] Loss: 0.182\n",
      "[51/1500] Loss: 0.181\n",
      "[51/1500] Validation log loss: 14.741\n",
      "[52/1500] Loss: 0.180\n",
      "[53/1500] Loss: 0.175\n",
      "[54/1500] Loss: 0.179\n",
      "[55/1500] Loss: 0.177\n",
      "[56/1500] Loss: 0.174\n",
      "[57/1500] Loss: 0.175\n",
      "[58/1500] Loss: 0.173\n",
      "[59/1500] Loss: 0.174\n",
      "[60/1500] Loss: 0.171\n",
      "[61/1500] Loss: 0.171\n",
      "[61/1500] Validation log loss: 15.452\n",
      "[62/1500] Loss: 0.172\n",
      "[63/1500] Loss: 0.169\n",
      "[64/1500] Loss: 0.168\n",
      "[65/1500] Loss: 0.169\n",
      "[66/1500] Loss: 0.168\n",
      "[67/1500] Loss: 0.168\n",
      "[68/1500] Loss: 0.166\n",
      "[69/1500] Loss: 0.162\n",
      "[70/1500] Loss: 0.163\n",
      "[71/1500] Loss: 0.162\n",
      "[71/1500] Validation log loss: 14.756\n",
      "[72/1500] Loss: 0.165\n",
      "[73/1500] Loss: 0.164\n",
      "[74/1500] Loss: 0.158\n",
      "[75/1500] Loss: 0.160\n",
      "[76/1500] Loss: 0.161\n",
      "[77/1500] Loss: 0.159\n",
      "[78/1500] Loss: 0.159\n",
      "[79/1500] Loss: 0.158\n",
      "[80/1500] Loss: 0.156\n",
      "[81/1500] Loss: 0.155\n",
      "[81/1500] Validation log loss: 14.562\n",
      "[82/1500] Loss: 0.158\n",
      "[83/1500] Loss: 0.156\n",
      "[84/1500] Loss: 0.153\n",
      "[85/1500] Loss: 0.155\n",
      "[86/1500] Loss: 0.153\n",
      "[87/1500] Loss: 0.153\n",
      "[88/1500] Loss: 0.154\n",
      "[89/1500] Loss: 0.155\n",
      "[90/1500] Loss: 0.151\n",
      "[91/1500] Loss: 0.150\n",
      "[91/1500] Validation log loss: 14.978\n",
      "[92/1500] Loss: 0.153\n",
      "[93/1500] Loss: 0.148\n",
      "[94/1500] Loss: 0.150\n",
      "[95/1500] Loss: 0.151\n",
      "[96/1500] Loss: 0.149\n",
      "[97/1500] Loss: 0.150\n",
      "[98/1500] Loss: 0.150\n",
      "[99/1500] Loss: 0.148\n",
      "[100/1500] Loss: 0.148\n",
      "[101/1500] Loss: 0.148\n",
      "[101/1500] Validation log loss: 14.576\n",
      "[102/1500] Loss: 0.147\n",
      "[103/1500] Loss: 0.145\n",
      "[104/1500] Loss: 0.148\n",
      "[105/1500] Loss: 0.146\n",
      "[106/1500] Loss: 0.145\n",
      "[107/1500] Loss: 0.143\n",
      "[108/1500] Loss: 0.144\n",
      "[109/1500] Loss: 0.144\n",
      "[110/1500] Loss: 0.141\n",
      "[111/1500] Loss: 0.145\n",
      "[111/1500] Validation log loss: 14.517\n",
      "[112/1500] Loss: 0.145\n",
      "[113/1500] Loss: 0.142\n",
      "[114/1500] Loss: 0.140\n",
      "[115/1500] Loss: 0.141\n",
      "[116/1500] Loss: 0.140\n",
      "[117/1500] Loss: 0.141\n",
      "[118/1500] Loss: 0.140\n",
      "[119/1500] Loss: 0.142\n",
      "[120/1500] Loss: 0.138\n",
      "[121/1500] Loss: 0.140\n",
      "[121/1500] Validation log loss: 14.170\n",
      "[122/1500] Loss: 0.139\n",
      "[123/1500] Loss: 0.139\n",
      "[124/1500] Loss: 0.137\n",
      "[125/1500] Loss: 0.138\n",
      "[126/1500] Loss: 0.138\n",
      "[127/1500] Loss: 0.137\n",
      "[128/1500] Loss: 0.136\n",
      "[129/1500] Loss: 0.137\n",
      "[130/1500] Loss: 0.136\n",
      "[131/1500] Loss: 0.137\n",
      "[131/1500] Validation log loss: 14.464\n",
      "[132/1500] Loss: 0.137\n",
      "[133/1500] Loss: 0.138\n",
      "[134/1500] Loss: 0.133\n",
      "[135/1500] Loss: 0.133\n",
      "[136/1500] Loss: 0.137\n",
      "[137/1500] Loss: 0.134\n",
      "[138/1500] Loss: 0.134\n",
      "[139/1500] Loss: 0.136\n",
      "[140/1500] Loss: 0.133\n",
      "[141/1500] Loss: 0.134\n",
      "[141/1500] Validation log loss: 14.165\n",
      "[142/1500] Loss: 0.134\n",
      "[143/1500] Loss: 0.135\n",
      "[144/1500] Loss: 0.132\n",
      "[145/1500] Loss: 0.132\n",
      "[146/1500] Loss: 0.133\n",
      "[147/1500] Loss: 0.131\n",
      "[148/1500] Loss: 0.132\n",
      "[149/1500] Loss: 0.132\n",
      "[150/1500] Loss: 0.132\n",
      "[151/1500] Loss: 0.133\n",
      "[151/1500] Validation log loss: 14.166\n",
      "[152/1500] Loss: 0.132\n",
      "[153/1500] Loss: 0.132\n",
      "[154/1500] Loss: 0.129\n",
      "[155/1500] Loss: 0.129\n",
      "[156/1500] Loss: 0.130\n",
      "[157/1500] Loss: 0.133\n",
      "[158/1500] Loss: 0.129\n",
      "[159/1500] Loss: 0.128\n",
      "[160/1500] Loss: 0.128\n",
      "[161/1500] Loss: 0.127\n",
      "[161/1500] Validation log loss: 15.003\n",
      "[162/1500] Loss: 0.127\n",
      "[163/1500] Loss: 0.129\n",
      "[164/1500] Loss: 0.129\n",
      "[165/1500] Loss: 0.125\n",
      "[166/1500] Loss: 0.130\n",
      "[167/1500] Loss: 0.125\n",
      "[168/1500] Loss: 0.125\n",
      "[169/1500] Loss: 0.127\n",
      "[170/1500] Loss: 0.129\n",
      "[171/1500] Loss: 0.124\n",
      "[171/1500] Validation log loss: 14.658\n",
      "[172/1500] Loss: 0.129\n",
      "[173/1500] Loss: 0.126\n",
      "[174/1500] Loss: 0.128\n",
      "[175/1500] Loss: 0.126\n",
      "[176/1500] Loss: 0.126\n",
      "[177/1500] Loss: 0.126\n",
      "[178/1500] Loss: 0.125\n",
      "[179/1500] Loss: 0.126\n",
      "[180/1500] Loss: 0.125\n",
      "[181/1500] Loss: 0.125\n",
      "[181/1500] Validation log loss: 15.413\n",
      "[182/1500] Loss: 0.125\n",
      "[183/1500] Loss: 0.125\n",
      "[184/1500] Loss: 0.122\n",
      "[185/1500] Loss: 0.122\n",
      "[186/1500] Loss: 0.123\n",
      "[187/1500] Loss: 0.124\n",
      "[188/1500] Loss: 0.124\n",
      "[189/1500] Loss: 0.124\n",
      "[190/1500] Loss: 0.122\n",
      "[191/1500] Loss: 0.124\n",
      "[191/1500] Validation log loss: 14.492\n",
      "[192/1500] Loss: 0.124\n",
      "[193/1500] Loss: 0.123\n",
      "[194/1500] Loss: 0.124\n",
      "[195/1500] Loss: 0.121\n",
      "[196/1500] Loss: 0.123\n",
      "[197/1500] Loss: 0.121\n",
      "[198/1500] Loss: 0.123\n",
      "[199/1500] Loss: 0.120\n",
      "[200/1500] Loss: 0.126\n",
      "[201/1500] Loss: 0.119\n",
      "[201/1500] Validation log loss: 14.581\n",
      "[202/1500] Loss: 0.121\n",
      "[203/1500] Loss: 0.119\n",
      "[204/1500] Loss: 0.121\n",
      "[205/1500] Loss: 0.123\n",
      "[206/1500] Loss: 0.119\n",
      "[207/1500] Loss: 0.121\n",
      "[208/1500] Loss: 0.119\n",
      "[209/1500] Loss: 0.120\n",
      "[210/1500] Loss: 0.119\n",
      "[211/1500] Loss: 0.119\n",
      "[211/1500] Validation log loss: 14.605\n",
      "[212/1500] Loss: 0.117\n",
      "[213/1500] Loss: 0.119\n",
      "[214/1500] Loss: 0.119\n",
      "[215/1500] Loss: 0.116\n",
      "[216/1500] Loss: 0.118\n",
      "[217/1500] Loss: 0.118\n",
      "[218/1500] Loss: 0.116\n",
      "[219/1500] Loss: 0.121\n",
      "[220/1500] Loss: 0.119\n",
      "[221/1500] Loss: 0.119\n",
      "[221/1500] Validation log loss: 15.175\n",
      "[222/1500] Loss: 0.117\n",
      "[223/1500] Loss: 0.116\n",
      "[224/1500] Loss: 0.116\n",
      "[225/1500] Loss: 0.117\n",
      "[226/1500] Loss: 0.118\n",
      "[227/1500] Loss: 0.116\n",
      "[228/1500] Loss: 0.115\n",
      "[229/1500] Loss: 0.116\n",
      "[230/1500] Loss: 0.117\n",
      "[231/1500] Loss: 0.118\n",
      "[231/1500] Validation log loss: 14.226\n",
      "[232/1500] Loss: 0.114\n",
      "[233/1500] Loss: 0.115\n",
      "[234/1500] Loss: 0.117\n",
      "[235/1500] Loss: 0.117\n",
      "[236/1500] Loss: 0.115\n",
      "[237/1500] Loss: 0.120\n",
      "[238/1500] Loss: 0.115\n",
      "[239/1500] Loss: 0.116\n",
      "[240/1500] Loss: 0.115\n",
      "[241/1500] Loss: 0.115\n",
      "[241/1500] Validation log loss: 14.502\n",
      "[242/1500] Loss: 0.117\n",
      "[243/1500] Loss: 0.117\n",
      "[244/1500] Loss: 0.114\n",
      "[245/1500] Loss: 0.112\n",
      "[246/1500] Loss: 0.114\n",
      "[247/1500] Loss: 0.115\n",
      "[248/1500] Loss: 0.114\n",
      "[249/1500] Loss: 0.113\n",
      "[250/1500] Loss: 0.113\n",
      "[251/1500] Loss: 0.115\n",
      "[251/1500] Validation log loss: 15.105\n",
      "[252/1500] Loss: 0.117\n",
      "[253/1500] Loss: 0.113\n",
      "[254/1500] Loss: 0.111\n",
      "[255/1500] Loss: 0.114\n",
      "[256/1500] Loss: 0.115\n",
      "[257/1500] Loss: 0.110\n",
      "[258/1500] Loss: 0.111\n",
      "[259/1500] Loss: 0.115\n",
      "[260/1500] Loss: 0.111\n",
      "[261/1500] Loss: 0.112\n",
      "[261/1500] Validation log loss: 15.100\n",
      "[262/1500] Loss: 0.115\n",
      "[263/1500] Loss: 0.113\n",
      "[264/1500] Loss: 0.112\n",
      "[265/1500] Loss: 0.112\n",
      "[266/1500] Loss: 0.113\n",
      "[267/1500] Loss: 0.111\n",
      "[268/1500] Loss: 0.112\n",
      "[269/1500] Loss: 0.111\n",
      "[270/1500] Loss: 0.115\n",
      "[271/1500] Loss: 0.110\n",
      "[271/1500] Validation log loss: 14.722\n",
      "[272/1500] Loss: 0.108\n",
      "[273/1500] Loss: 0.112\n",
      "[274/1500] Loss: 0.110\n",
      "[275/1500] Loss: 0.107\n",
      "[276/1500] Loss: 0.113\n",
      "[277/1500] Loss: 0.107\n",
      "[278/1500] Loss: 0.110\n",
      "[279/1500] Loss: 0.112\n",
      "[280/1500] Loss: 0.109\n",
      "[281/1500] Loss: 0.112\n",
      "[281/1500] Validation log loss: 14.356\n",
      "[282/1500] Loss: 0.108\n",
      "[283/1500] Loss: 0.111\n",
      "[284/1500] Loss: 0.109\n",
      "[285/1500] Loss: 0.110\n",
      "[286/1500] Loss: 0.110\n",
      "[287/1500] Loss: 0.107\n",
      "[288/1500] Loss: 0.109\n",
      "[289/1500] Loss: 0.110\n",
      "[290/1500] Loss: 0.109\n",
      "[291/1500] Loss: 0.111\n",
      "[291/1500] Validation log loss: 14.278\n",
      "[292/1500] Loss: 0.109\n",
      "[293/1500] Loss: 0.111\n",
      "[294/1500] Loss: 0.111\n",
      "[295/1500] Loss: 0.108\n",
      "[296/1500] Loss: 0.106\n",
      "[297/1500] Loss: 0.109\n",
      "[298/1500] Loss: 0.110\n",
      "[299/1500] Loss: 0.110\n",
      "[300/1500] Loss: 0.109\n",
      "[301/1500] Loss: 0.107\n",
      "[301/1500] Validation log loss: 13.766\n",
      "[302/1500] Loss: 0.110\n",
      "[303/1500] Loss: 0.108\n",
      "[304/1500] Loss: 0.106\n",
      "[305/1500] Loss: 0.110\n",
      "[306/1500] Loss: 0.109\n",
      "[307/1500] Loss: 0.107\n",
      "[308/1500] Loss: 0.106\n",
      "[309/1500] Loss: 0.106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[310/1500] Loss: 0.111\n",
      "[311/1500] Loss: 0.106\n",
      "[311/1500] Validation log loss: 14.942\n",
      "[312/1500] Loss: 0.107\n",
      "[313/1500] Loss: 0.108\n",
      "[314/1500] Loss: 0.106\n",
      "[315/1500] Loss: 0.107\n",
      "[316/1500] Loss: 0.107\n",
      "[317/1500] Loss: 0.107\n",
      "[318/1500] Loss: 0.106\n",
      "[319/1500] Loss: 0.105\n",
      "[320/1500] Loss: 0.108\n",
      "[321/1500] Loss: 0.107\n",
      "[321/1500] Validation log loss: 14.610\n",
      "[322/1500] Loss: 0.106\n",
      "[323/1500] Loss: 0.103\n",
      "[324/1500] Loss: 0.107\n",
      "[325/1500] Loss: 0.109\n",
      "[326/1500] Loss: 0.107\n",
      "[327/1500] Loss: 0.103\n",
      "[328/1500] Loss: 0.105\n",
      "[329/1500] Loss: 0.104\n",
      "[330/1500] Loss: 0.106\n",
      "[331/1500] Loss: 0.104\n",
      "[331/1500] Validation log loss: 14.404\n",
      "[332/1500] Loss: 0.105\n",
      "[333/1500] Loss: 0.106\n",
      "[334/1500] Loss: 0.106\n",
      "[335/1500] Loss: 0.107\n",
      "[336/1500] Loss: 0.105\n",
      "[337/1500] Loss: 0.103\n",
      "[338/1500] Loss: 0.103\n",
      "[339/1500] Loss: 0.104\n",
      "[340/1500] Loss: 0.103\n",
      "[341/1500] Loss: 0.104\n",
      "[341/1500] Validation log loss: 14.536\n",
      "[342/1500] Loss: 0.103\n",
      "[343/1500] Loss: 0.105\n",
      "[344/1500] Loss: 0.104\n",
      "[345/1500] Loss: 0.107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define classifier class, architecture of nn\n",
    "class _classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_classifier, self).__init__()\n",
    "        self.emb_layers = emb_layers\n",
    "        self.first_bn_layer = bn1\n",
    "        self.main = nn.Sequential(\n",
    "\n",
    "            linear1,leakyrelu,nn.BatchNorm1d(hiddenLayer1Size),dropout2,\n",
    "            linear2,leakyrelu,nn.BatchNorm1d(hiddenLayer2Size),dropout,          \n",
    "            linear3,leakyrelu,nn.BatchNorm1d(hiddenLayer3Size),dropout,\n",
    "            linear4,leakyrelu,nn.BatchNorm1d(hiddenLayer4Size),dropout,\n",
    "            linear5,sigmoid\n",
    "            \n",
    "        )\n",
    "#define pytorch forward pass        \n",
    "    def forward(self, cat_data, cont_data):\n",
    "        x = [emb_layer(cat_data[:, i]) for i,emb_layer in enumerate(self.emb_layers)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = dropout1(x)\n",
    "        normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "        mainin = torch.cat([x, normalized_cont_data], 1) \n",
    "        \n",
    "        return self.main(mainin)\n",
    "\n",
    "classifier = _classifier().cuda()\n",
    "#define optimizer and criterion, we dont use a LR sheduler for now\n",
    "optimizer = optim.Adam(classifier.parameters())\n",
    "#lr=LR,weight_decay=5e-3 learning rate and weight decay not implemented yet\n",
    "criterion = nn.BCELoss()\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60,100,150,400], gamma = 0.1)\n",
    "\n",
    "#train network with n epochs and minibatches\n",
    "epochs = 1500\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    tu = [] \n",
    "    \n",
    "    for sample_batched, labels_batched, cont_data in dataloader:\n",
    "          \n",
    "        output = classifier(sample_batched.cuda(),cont_data.cuda()) # predict labels from input\n",
    "        loss = criterion(output.cuda(), labels_batched.cuda()) #compute loss\n",
    "\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward() # backpropagation, compute gradients\n",
    "        optimizer.step() # apply gradients\n",
    "        losses.append(loss.data.mean())\n",
    "    #scheduler.step() #apply scheduler after each epoch\n",
    "        \n",
    "    print('[%d/%d] Loss: %.3f' % (epoch+1, epochs, np.mean(losses)))\n",
    "\n",
    "     \n",
    "    if epoch % 10 == 0:\n",
    "  #check validation log loss every 10 epoch  \n",
    "        cl1 = classifier\n",
    "        prediction = (cl1(X_tensor_val.cuda(),cont_data_val.cuda()).data > 0.5).float() # zero or one\n",
    "        pred_y = prediction.cpu().numpy().squeeze()\n",
    "        target_y = Y_tensor_val.cpu().data.numpy()\n",
    "        tu.append(log_loss(target_y, pred_y))\n",
    "        print('[%d/%d] Validation log loss: %.3f' % (epoch+1, epochs, np.mean(tu)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicitons for whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = classifier\n",
    "\n",
    "#prediction = (cl1(X_tensor_val).data).float() # probabilities \n",
    "prediction = (cl1(X_tensor_val.cuda(),cont_data_val.cuda()).data > 0.5).float() # zero or one\n",
    "\n",
    "pred_y = prediction.cpu().numpy().squeeze()\n",
    "\n",
    "target_y = Y_tensor_val.cpu().data.numpy()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "labels = ['0', '1','2']\n",
    "cm = confusion_matrix(\n",
    "    target_y.argmax(axis=1), pred_y.argmax(axis=1))\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "#cplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save prediction for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = (cl1(X_tensor_test.cuda(),cont_data_test.cuda()).data > 0.5).float() # zero or one\n",
    "pred_y_test = prediction_test.cpu().numpy().squeeze()\n",
    "\n",
    "IDtest = pd.DataFrame(data=X_test_1.index.values,columns = ['Unfall_ID'])\n",
    "pred = pd.Series(pred_y_test.argmax(axis=1), name=\"Unfallschwere\")\n",
    "\n",
    "results = pd.concat([IDtest,pred],axis=1)\n",
    "\n",
    "results.to_csv(\"MLP_pytorch_embed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using different classical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels instead of one-hot encoding\n",
    "Y_train1 = Y_train.values\n",
    "Y_train1  = np.argmax(Y_train1, axis=1)\n",
    "Y_train1\n",
    "\n",
    "# Try different models, combine them later in ensemble learning, use 10 folds\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting\n",
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300],\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] }\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "gsGBC.fit(X_train,Y_train1)\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = pd.Series(GBC_best.predict(X_train), name=\"GBC\")\n",
    "target_y = Y_train1\n",
    "\n",
    "labels = ['0', '1','2']\n",
    "cm = confusion_matrix(\n",
    "    target_y, pred_y)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = GBC_best.predict(x_test)\n",
    "target_y = np.argmax(y_test.values, axis=1)\n",
    "cm = confusion_matrix(\n",
    "    target_y, pred_y)\n",
    "\n",
    "print(cm)\n",
    "labels = ['0', '1','2']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTrees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtC = ExtraTreesClassifier()\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 7],\n",
    "              \"min_samples_split\": [2, 3, 7],\n",
    "              \"min_samples_leaf\": [1, 3, 7],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[300,600],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "gsExtC.fit(X_train,Y_train1)\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 7],\n",
    "              \"min_samples_split\": [2, 3, 7],\n",
    "              \"min_samples_leaf\": [1, 3, 7],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[300,600],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "gsrandom_forest = GridSearchCV(random_forest,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "gsrandom_forest.fit(X_train,Y_train1)\n",
    "# Best score\n",
    "random_forest_best = gsrandom_forest.best_estimator_\n",
    "gsrandom_forest.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = pd.Series(random_forest_best.predict(X_train), name=\"random_forest\")\n",
    "target_y = Y_train1\n",
    "labels = ['0', '1','2']\n",
    "cm = confusion_matrix(\n",
    "    target_y, pred_y)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = pd.Series(random_forest_best.predict(x_test), name=\"random_forest\")\n",
    "target_y = np.argmax(y_test.values, axis=1)\n",
    "cm = confusion_matrix(\n",
    "    target_y, pred_y)\n",
    "\n",
    "print(cm)\n",
    "labels = ['0', '1','2']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier()\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "gsadaDTC.fit(X_train,Y_train1)\n",
    "adaDTC_best = gsadaDTC.best_estimator_\n",
    "gsadaDTC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMC = SVC(probability=True)\n",
    "svc_param_grid = {'kernel': ['rbf'], \n",
    "                  'gamma': [ 0.001, 0.1],\n",
    "                  'C': [10,200]}\n",
    "gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "gsSVMC.fit(X_train,Y_train1)\n",
    "SVMC_best = gsSVMC.best_estimator_\n",
    "# Best score\n",
    "gsSVMC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_Unfallschwere_AdaDTC = pd.Series(adaDTC_best.predict(X_test_1), name=\"AdaDTC\")\n",
    "test_Unfallschwere_ExtC = pd.Series(ExtC_best.predict(X_test_1), name=\"ExtC\")\n",
    "test_Unfallschwere_GBC = pd.Series(GBC_best.predict(X_test_1), name=\"GBC\")\n",
    "test_Unfallschwere_SVMC = pd.Series(SVMC_best.predict(X_test_1), name=\"SVMC\")\n",
    "test_Unfallschwere_random_forest = pd.Series(random_forest_best.predict(X_test_1), name=\"random_forest\")\n",
    "\n",
    "\n",
    "# Concatenate all classifier results\n",
    "ensemble_results = pd.concat([test_Unfallschwere_AdaDTC, test_Unfallschwere_ExtC, test_Unfallschwere_GBC,test_Unfallschwere_SVMC,test_Unfallschwere_random_forest],axis=1)\n",
    "\n",
    "VotingPredictor = VotingClassifier(estimators=[('ExtC', ExtC_best), ('GBC',GBC_best),\n",
    "('SVMC', SVMC_best), ('random_forest', random_forest_best)], voting='soft', n_jobs=4)\n",
    "VotingPredictor = VotingPredictor.fit(X_train, Y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save prediciton from ensemble voting\n",
    "IDtest = pd.DataFrame(data=X_test.index.values,columns = ['Unfall_ID'])\n",
    "test = pd.Series(VotingPredictor.predict(X_test_1), name=\"Unfallschwere\")\n",
    "\n",
    "results = pd.concat([IDtest,test],axis=1)\n",
    "\n",
    "results.to_csv(\"ensemble_python_voting.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
